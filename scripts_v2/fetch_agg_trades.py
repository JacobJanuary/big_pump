"""
–ó–∞–≥—Ä—É–∑–∫–∞ Binance aggTrades (daily dumps) –¥–ª—è Delta Reversal –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞.

–°–∫–∞—á–∏–≤–∞–µ—Ç daily ZIP —Ñ–∞–π–ª—ã, —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç 48—á –æ–∫–Ω–æ –ø–æ—Å–ª–µ —Å–∏–≥–Ω–∞–ª–∞, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤ –ë–î.
"""
import os
import sys
import zipfile
import csv
from pathlib import Path
from datetime import datetime, timezone, timedelta
from collections import defaultdict
import requests
import hashlib

# Add scripts directory to path
current_dir = Path(__file__).resolve().parent
sys.path.append(str(current_dir))

from pump_analysis_lib import get_db_connection

# Configuration
BASE_URL = "https://data.binance.vision"
DATA_DIR = Path(__file__).resolve().parent.parent / "data" / "agg_trades"
FUTURES_PATH = "data/futures/um/daily/aggTrades"

def get_signals_for_loading(conn, limit=None):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–∏–≥–Ω–∞–ª—ã –∏–∑ web.signal_analysis."""
    query = """
        SELECT 
            sa.id,
            sa.pair_symbol,
            sa.signal_timestamp,
            sa.entry_time
        FROM web.signal_analysis sa
        ORDER BY sa.signal_timestamp ASC
    """
    if limit:
        query += f" LIMIT {limit}"
    
    with conn.cursor() as cur:
        cur.execute(query)
        rows = cur.fetchall()
    
    return [{'id': r[0], 'pair_symbol': r[1], 'signal_timestamp': r[2], 'entry_time': r[3]} for r in rows]

def get_required_dates(signal_timestamp):
    """
    –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞–∫–∏–µ –¥–Ω–∏ –Ω—É–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å –¥–ª—è 48—á –æ–∫–Ω–∞.
    
    Returns: list of date strings ['2025-01-01', '2025-01-02', '2025-01-03']
    """
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ UTC –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if signal_timestamp.tzinfo is None:
        signal_timestamp = signal_timestamp.replace(tzinfo=timezone.utc)
    
    start_date = signal_timestamp.date()
    end_date = (signal_timestamp + timedelta(hours=48)).date()
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞—Ç—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
    dates = []
    current = start_date
    while current <= end_date:
        dates.append(current.strftime('%Y-%m-%d'))
        current += timedelta(days=1)
    
    return dates

def download_daily_file(symbol: str, date: str) -> Path | None:
    """
    –°–∫–∞—á–∞—Ç—å daily aggTrades ZIP —Ñ–∞–π–ª.
    
    Returns: Path to downloaded file or None if failed.
    """
    filename = f"{symbol}-aggTrades-{date}.zip"
    url = f"{BASE_URL}/{FUTURES_PATH}/{symbol}/{filename}"
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    symbol_dir = DATA_DIR / symbol
    symbol_dir.mkdir(parents=True, exist_ok=True)
    dest_path = symbol_dir / filename
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —É–∂–µ
    if dest_path.exists():
        print(f"    ‚è≠Ô∏è –£–∂–µ —Å–∫–∞—á–∞–Ω: {filename}")
        return dest_path
    
    try:
        print(f"    üì• –°–∫–∞—á–∏–≤–∞—é: {filename}...", end=' ', flush=True)
        response = requests.get(url, stream=True, timeout=300)
        
        if response.status_code == 404:
            print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω")
            return None
        
        response.raise_for_status()
        
        with open(dest_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        size_mb = dest_path.stat().st_size / 1024 / 1024
        print(f"‚úÖ {size_mb:.1f} MB")
        return dest_path
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return None

def extract_and_filter_trades(zip_path: Path, start_ms: int, end_ms: int):
    """
    –†–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å ZIP –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Ç—Ä–µ–π–¥—ã –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –æ–∫–Ω—É.
    
    CSV —Ñ–æ—Ä–º–∞—Ç: agg_trade_id,price,quantity,first_trade_id,last_trade_id,transact_time,is_buyer_maker
    
    Returns: list of trade dicts
    """
    trades = []
    
    try:
        with zipfile.ZipFile(zip_path, 'r') as zf:
            # –í –∞—Ä—Ö–∏–≤–µ –æ–¥–∏–Ω CSV —Ñ–∞–π–ª
            csv_filename = zf.namelist()[0]
            
            with zf.open(csv_filename) as f:
                # –ß–∏—Ç–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç
                import io
                text_file = io.TextIOWrapper(f, encoding='utf-8')
                reader = csv.reader(text_file)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                next(reader, None)
                
                for row in reader:
                    # agg_trade_id,price,quantity,first_trade_id,last_trade_id,transact_time,is_buyer_maker
                    transact_time = int(row[5])
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                    if start_ms <= transact_time <= end_ms:
                        trades.append({
                            'agg_trade_id': int(row[0]),
                            'price': float(row[1]),
                            'quantity': float(row[2]),
                            'transact_time': transact_time,
                            'is_buyer_maker': row[6].lower() == 'true'
                        })
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è ZIP: {e}")
    
    return trades

def insert_trades(conn, signal_id: int, pair_symbol: str, trades: list):
    """–í—Å—Ç–∞–≤–∏—Ç—å —Ç—Ä–µ–π–¥—ã –≤ web.agg_trades."""
    if not trades:
        return 0
    
    with conn.cursor() as cur:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º executemany –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        cur.executemany("""
            INSERT INTO web.agg_trades 
                (signal_analysis_id, pair_symbol, agg_trade_id, price, quantity, transact_time, is_buyer_maker)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
        """, [
            (signal_id, pair_symbol, t['agg_trade_id'], t['price'], t['quantity'], t['transact_time'], t['is_buyer_maker'])
            for t in trades
        ])
    
    return len(trades)

def process_signal(sig):
    """
    –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω —Å–∏–≥–Ω–∞–ª (–¥–ª—è multiprocessing).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (signal_id, trades_list) –∏–ª–∏ (signal_id, None) –ø—Ä–∏ –æ—à–∏–±–∫–µ.
    """
    signal_id = sig['id']
    symbol = sig['pair_symbol']
    signal_ts = sig['signal_timestamp']
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ (48—á –ø–æ—Å–ª–µ —Å–∏–≥–Ω–∞–ª–∞)
    if signal_ts.tzinfo is None:
        signal_ts = signal_ts.replace(tzinfo=timezone.utc)
    
    start_ms = int(signal_ts.timestamp() * 1000)
    end_ms = int((signal_ts + timedelta(hours=48)).timestamp() * 1000)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω—ã–µ –¥–∞—Ç—ã
    dates = get_required_dates(signal_ts)
    
    # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã
    all_trades = []
    for date in dates:
        zip_path = download_daily_file(symbol, date)
        if zip_path:
            trades = extract_and_filter_trades(zip_path, start_ms, end_ms)
            all_trades.extend(trades)
    
    if not all_trades:
        return (signal_id, symbol, None)
    
    return (signal_id, symbol, all_trades)

def fetch_agg_trades(limit=None, dry_run=False, workers=12):
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: —Å–∫–∞—á–∞—Ç—å –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å aggTrades –¥–ª—è –≤—Å–µ—Ö —Å–∏–≥–Ω–∞–ª–æ–≤.
    """
    from multiprocessing import Pool
    
    print("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ AggTrades (Daily Dumps) - 48—á –æ–∫–Ω–æ")
    print(f"   –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {DATA_DIR}")
    print(f"   –í–æ—Ä–∫–µ—Ä–æ–≤: {workers}")
    print("-" * 60)
    
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    try:
        with get_db_connection() as conn:
            signals = get_signals_for_loading(conn, limit=limit)
            
            if not signals:
                print("‚úÖ –í—Å–µ —Å–∏–≥–Ω–∞–ª—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
                return
            
            print(f"–ù–∞–π–¥–µ–Ω–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(signals)}")
            print("-" * 60)
            
            total_trades = 0
            processed = 0
            failed = 0
            
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
            with Pool(processes=workers) as pool:
                results = pool.map(process_signal, signals)
            
            # –í—Å—Ç–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ë–î –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
            for i, (signal_id, symbol, trades) in enumerate(results, 1):
                if trades is None:
                    print(f"[{i}/{len(signals)}] {symbol} - ‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                    failed += 1
                    continue
                
                if not dry_run:
                    inserted = insert_trades(conn, signal_id, symbol, trades)
                    conn.commit()
                    total_trades += inserted
                    print(f"[{i}/{len(signals)}] {symbol} - ‚úÖ {inserted:,} —Ç—Ä–µ–π–¥–æ–≤")
                else:
                    print(f"[{i}/{len(signals)}] {symbol} - [DRY RUN] {len(trades):,} —Ç—Ä–µ–π–¥–æ–≤")
                
                processed += 1
            
            print("\n" + "=" * 60)
            print(f"üìä –ò—Ç–æ–≥–æ:")
            print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {processed}")
            print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {failed}")
            print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç—Ä–µ–π–¥–æ–≤: {total_trades:,}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–ó–∞–≥—Ä—É–∑–∫–∞ Binance aggTrades')
    parser.add_argument('--limit', type=int, default=None, help='–õ–∏–º–∏—Ç —Å–∏–≥–Ω–∞–ª–æ–≤')
    parser.add_argument('--dry-run', action='store_true', help='–¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ')
    parser.add_argument('--workers', type=int, default=12, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤')
    
    args = parser.parse_args()
    
    fetch_agg_trades(limit=args.limit, dry_run=args.dry_run, workers=args.workers)

